{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Om\\Anaconda3\\envs\\opencv\\lib\\site-packages\\keras\\engine\\saving.py:341: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "trial_model=load_model(\"C:\\\\Users\\\\Om\\\\Desktop\\\\Cerberus\\\\Utilities\\\\VGGFace.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features(featmodel, crpimg, transform=False): #extract face feature vector\n",
    "    \n",
    "    # transform=True seems more robust but I think the RGB channels are not in right order\n",
    "    \n",
    "    imarr = np.array(crpimg).astype(np.float32)\n",
    "\n",
    "    if transform:\n",
    "        imarr[:,:,0] -= 129.1863\n",
    "        imarr[:,:,1] -= 104.7624\n",
    "        imarr[:,:,2] -= 93.5940        \n",
    "        aux = copy.copy(imarr)\n",
    "    imarr = np.expand_dims(imarr, axis=0)\n",
    "    output= featmodel.predict(imarr)[0,:]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extractor(inpDir,personID,wordID,timestep): #access directory and extract face vector     \n",
    "        \n",
    "    sample_space=[] #for appending different utterances\n",
    "    feature_sequence = [] # for appending different timesteps\n",
    "    inpDir=inpDir #input dataset\n",
    "    linpDir = os.listdir(inpDir) #list all directories in dataset\n",
    "    #print(linpDir)\n",
    "    personStr= linpDir[personID]\n",
    "    personFolder = '%s\\\\%s\\\\words' % (inpDir,personStr) #opening words folder\n",
    "    lpersonFolder = os.listdir(personFolder)\n",
    "    wordID= lpersonFolder[wordID]\n",
    "    wordFolder = '%s\\\\%s' % (personFolder,wordID)\n",
    "    lwordFolder = os.listdir(wordFolder)\n",
    "    i=0\n",
    "    for utterance in lwordFolder:\n",
    "        \n",
    "        utterFolder= '%s\\\\%s' % (wordFolder,utterance)\n",
    "        lutterFolder = os.listdir(utterFolder)\n",
    "        colourFolder ='%s\\\\%s' % (utterFolder,lutterFolder[0])\n",
    "        lcolourFolder = os.listdir(colourFolder)\n",
    "        frame = lcolourFolder[timestep]\n",
    "        i += 1\n",
    "        image= \"%s\\\\%s\" % (colourFolder,frame) \n",
    "        im=Image.open(image)\n",
    "        im = im.resize((224,224))\n",
    "        feature_vector = features(trial_model,im, transform=True)\n",
    "        if i==1 :\n",
    "            feature_sequence=feature_vector #done because of need of same dim. for concatenation\n",
    "        else:\n",
    "            feature_sequence = np.concatenate((feature_sequence,feature_vector),axis=0)\n",
    "            \n",
    "   \n",
    "    return feature_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_list_for_training=[1,2,10,11,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def people_extractor(inpDir,person_list):\n",
    "    people_array=[]\n",
    "    dim_count=0\n",
    "    for people in range(0,len(person_list)):\n",
    "        feature_sequence=feature_extractor(inpDir,person_list[people],0,0)\n",
    "        if dim_count==0:\n",
    "            people_array=feature_sequence\n",
    "        else:\n",
    "            people_array=np.concatenate((people_array,feature_sequence))\n",
    "        dim_count = dim_count + 1\n",
    "        print(people_array.shape)\n",
    "    return people_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(26220,)\n",
      "(52440,)\n",
      "(78660,)\n",
      "(104880,)\n",
      "(131100,)\n"
     ]
    }
   ],
   "source": [
    "people_array=people_extractor(\"C:\\\\Users\\\\Om\\\\Desktop\\\\dataset\",people_list_for_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(131100,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "people_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"Training_array1.npy\",people_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
